
# primalstep：目标到可执行步骤的智能转化器

## 产品需求文档 (PRD) - 版本 1.0

### 1. 引言

本PRD旨在详细定义一个创新的Python包 `primalstep`，该包能够将用户提供的高层级、模糊目标智能地分解为一系列明确、可执行的自然语言步骤，并清晰地展现这些步骤之间的因果及前置依赖关系。`primalstep` 将利用大型语言模型（LLM）的强大能力进行自动化分解，旨在帮助用户将复杂的任务变得易于管理和执行。

### 2. 目标与愿景

*   **核心目标：** 自动化地将一个高层级目标（字符串形式）分解为一系列粒度适中（约30分钟工作量）、可执行的自然语言步骤。
*   **关键特性：** 明确提供步骤间的因果关系和前置依赖，以图结构（NetworkX）表示。
*   **愿景：** 成为个人和团队任务规划、项目管理和自动化工作流设计的智能辅助工具，提高任务执行效率和成功率。

### 3. 用户故事

*   **作为一名普通用户，** 我想把“我想减肥”这样的模糊目标，分解成“制定饮食计划”、“每天跑步30分钟”等一系列具体、可操作的小步骤，并知道它们之间谁先谁后，这样我就可以按部就班地执行。
*   **作为一名游戏玩家，** 我想把“在游戏里做一个万塘工程”这样的宏大目标，分解成“规划地形”、“挖掘水渠”、“种植水生植物”等具体步骤，并理解它们的依赖关系，以便高效完成项目。
*   **作为一名开发者，** 我想通过API或命令行界面，集成这个任务分解能力到我的应用中，例如用于自动化工作流的步骤生成或智能客服的引导。
*   **作为一名系统管理员，** 我希望分解出的步骤中能包含机器可识别的指令，以便我的自动化系统能够直接执行部分任务。

### 4. 功能需求

#### 4.1 核心功能：任务分解

*   **输入：** 接收一个字符串形式的高层级目标（例如：“我想减肥”，“在游戏里做一个万塘工程”）。
*   **处理：**
    *   利用大型语言模型（LLM）对输入目标进行语义理解和分解。
    *   分解粒度：确保每个分解出的步骤是可执行的，且任务量大致控制在约30分钟。
    *   生成自然语言步骤：每个步骤以清晰的自然语言描述。
    *   识别和嵌入机器指令：分解出的步骤中可以包含特定格式的机器可识别指令（例如：`[ACTION: RUN_PROGRAM "script.py"]`），这些指令与自然语言描述融合。
    *   推理依赖关系：LLM需推理出步骤间的因果和前置依赖关系。
*   **输出：**
    *   一个NetworkX有向无环图（DAG），表示步骤间的依赖关系。
    *   一个包含所有步骤详细信息的字典，每个步骤包含：
        *   `id`：唯一标识符。
        *   `description`：自然语言描述。
        *   `instructions`：可选的机器可识别指令列表。
        *   （依赖关系通过NetworkX图表示，但输出字典中也可冗余包含方便查阅）。

#### 4.2 接口与交互

*   **Python Package API：** 提供核心的 `TaskDecomposer` 类，作为包的公共接口，所有对外功能通过 `primalstep/core.py` 暴露。
*   **命令行接口 (CLI)：**
    *   提供一个命令行工具 (`cli.py`)，允许用户通过命令行输入目标并获取分解结果。
    *   支持文本和JSON两种输出格式。
    *   支持选择使用真实LLM或模拟LLM。
    *   启动方式：通过 `pyproject.toml` 定义的脚本入口。
*   **Web API (FastAPI)：**
    *   提供一个HTTP RESTful API服务 (`primalstep/server.py`)，允许其他系统通过HTTP请求调用任务分解功能。
    *   API 端点：`/decompose`（POST 请求）。
    *   请求体：包含 `goal` 字符串。
    *   响应体：包含分解后的图结构（节点、边）和步骤详情（JSON格式）。
    *   提供Swagger UI/OpenAPI 文档。
    *   启动方式：通过直接运行 `primalstep/server.py` 或通过 `uvicorn` 命令。

### 5. 非功能性需求

*   **性能：**
    *   API响应时间：在模拟模式下应快速响应（毫秒级）。真实LLM模式下，响应时间取决于LLM提供商的API延迟。
    *   CLI响应时间：与API类似。
*   **可靠性：**
    *   健壮的错误处理：能够处理LLM返回无效JSON、解析失败、循环依赖等情况。
    *   高可用性：FastAPI服务应具备一定的健壮性，能够处理并发请求（取决于部署方式）。
*   **可扩展性：**
    *   易于集成不同的LLM：通过抽象接口设计，方便切换或添加新的LLM后端（如OpenAI, Gemini, Llama等）。
    *   易于扩展新的指令类型：机器可识别指令的格式应可配置和扩展。
    *   模块化设计：内部模块职责分离，便于功能扩展和维护。
*   **可维护性：**
    *   清晰的代码结构和命名规范。
    *   详细的文档注释。
*   **可测试性：**
    *   所有核心功能和模块都应易于进行单元测试和集成测试。
    *   提供Mock机制，便于在不依赖外部服务的情况下进行测试。
*   **安全性：**
    *   API Key等敏感信息不应硬编码，应通过环境变量或安全配置管理。
    *   FastAPI服务应考虑基本的输入校验和错误处理，防止常见Web漏洞。
*   **日志：** 系统应具备完善的日志记录功能，由 `primalstep/log.py` 提供单例 `Log` 类实现。
    *   支持不同日志级别（DEBUG, INFO, WARNING, ERROR, CRITICAL）。
    *   日志输出到控制台和按大小轮转的文件 (`logs/app.log`)。
    *   日志级别可根据运行环境（开发/生产）通过 `Log.reset_level()` 进行配置。
    *   日志格式：`时间 - 日志级别 - Logger名称 - 模块名:行号 - 消息`。
*   **配置管理：**
    *   FastAPI 服务端口和环境（`dev`/`prod`）应支持命令行参数配置，并提供默认值。
    *   开发环境默认端口为 8008，生产环境为 8008。通过命令行参数 `--port` 可指定。
    *   开发环境 (`--env dev`) 端口默认会偏移 +100，即 8108。
*   **可部署性：** 包应易于打包和分发（例如 `.whl`, `.tar.gz`），并提供标准入口点。

### 6. 技术选型

*   **核心语言：** Python
*   **任务分解逻辑：** 大型语言模型 (LLM)
*   **图结构：** NetworkX
*   **Web框架：** FastAPI
*   **命令行接口：** Click
*   **日志：** Python内置 `logging` 模块 (通过 `primalstep/log.py` 中的 `Log` 类封装)
*   **依赖管理：** `pyproject.toml` (Poetry/Hatch/Flit 等工具)
*   **HTTP服务器：** Uvicorn
*   **测试框架：** pytest
*   **文档生成：** MkDocs (根据 `mkdocs.yml`)

### 7. 迭代与发布计划 (示例)

*   **MVP (Minimum Viable Product) 阶段：**
    *   实现核心 `TaskDecomposer` 类。
    *   实现 `BaseLLMClient` 抽象类和 `MockLLMClient`。
    *   CLI 工具：支持基本分解和文本/JSON输出。
    *   FastAPI 服务：支持 `/decompose` API 端点，基于提供的 `primalstep/server.py` 模板完成。
    *   日志功能：基于提供的 `primalstep/log.py` 实现。
    *   基本错误处理和DAG验证。
    *   完善 `pyproject.toml` 配置。
*   **第二阶段：**
    *   集成一个真实的LLM客户端（如OpenAIClient）。
    *   优化LLM提示工程，提高分解质量和依赖推理准确性。
    *   更细致的错误处理和日志记录。
    *   完善 `primalstep` 文档和使用示例。
    *   明确 `main.py` 的作用，如无必要则移除。
*   **后续阶段：**
    *   支持更多LLM后端。
    *   增强指令识别和执行框架。
    *   增加任务优先级、负责人、预计耗时等元数据支持。
    *   Web界面可视化任务图。
    *   高级错误恢复和重试机制。

